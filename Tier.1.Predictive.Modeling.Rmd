---
title: "Tier.1.Machine.Learning"
author: "Annamalai Muthiah"
date: "Oct 30, 2020"
output: 
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<b> Introduction </b> In this article, I explain and demonstrate a few basic principles of machine learning (I call it "Tier 1" of Machine Learning) using the programming language R and the algae data set included in the R code package, DMwR. The variable to be predicted through the model (also called "target variable" or "response variable") is the algae level,  a1.  The other set of variables used in the model are called the "predictor variables" (also called "feature variables", that is, features in the data) that will be part of the mathematical model or function that will predict the Target Variable. 

There are three major steps in machine learning: 

<b> Step 1. Data Clean-up </b> This step helps to clean-up irregular and missing values in the data set before conducting further analysis

<b> Step 2. Exploratory Data Analysis </b> Exploring variables of interest in the data set through visualization tools so as to form initial impression of the data set

<b> Step 3: Mathematical Model (Machine Learning Model) setup </b> 
There are two subsets within this step.  
<i>Step 3.1. Variable Selection Step.</i> Since there are many variables in the data, it is not possible to include all of them in the mathematical model.It is necessary to get a list of the most important variable that can then be used to form the model. This step is called "variable selection" step. One way is to make a linear regression model. There are few other methods you could use for variable selection step - bootstrapping, cross validation, stepwise and subset regression. These concepts are discussed in more detail below.  
<i>Step 3.2.</i> Construction of predictive modeling.

This article's focus will be on Steps 1 to 3.1. I will post another article to focus on Step 3.2 since it a big topic.

# How to set up the data and code packages in R?

As mentioned above, code package not only contain useful code fragments made available but also contains datasets within them. 
As stated above, DMwE package contains algae data set. The below code fragment shows how to install and attach the package to your R environment.

```{r pkg_installation, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
# install.packages("DMwR", dependencies=TRUE)
library(DMwR)
``` 
There are additional packages such as lattice, grids etc. that will get installed alongside DMwR

## Step 1. Clean-up dataset

*** The goal of this step is to conduct Summary of dataset to expose NA and other irregular characters in the data set ***

The following str() command displays the data types (numerical/float, integer, categorical, string etc.) of variables well. 
'data.frame' is a special matrix or a table that can contain different data types together.
The data set consists of 3 categorical variables (also called the "factor" data type in R - season, size, speed) and 15 numerical variables.

```{r structure, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
str(algae)
```

```{r data.summary, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
# The following command will effectively summarize the data range.
# As can be seen, there are a few missing values indicated by "Not Available (NA)" in the data
summary(algae)
```

Now that missing values (NAs) have been located in the dataset, it is time to address them because it will hinder model estimation  
<b> Step# 1.1: Remove observations in the data that have "many" missing values </b>  
In this example, I set the criterion for "many" to be >=20% values to be missing values)  
<b> Step# 1.2:  Impute potential values for the missing values from 'neighboring' observations </b>  
'Neighboring' observation to a given observation is defined in terms of their numerical correlations.

Step 1.1: Removing observations with too "many" missing values.   
I define them as "non-ideal.observations".manyNAs is a function in the DMwR code package that helps to identify the non-ideal observations, observations that have 20% or more # values to be missing. The function accepts algae (the dataset/dataframe) as input and 20% is encoded as 0.2 in the formula of the manyNAs function.manyNAs output the row numbers/IDs of "non-ideal.observations" in the dataframe.
```{r Step1.1, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
# Displays a list of non-ideal observations
manyNAs (algae, 0.2) 
# These non-ideal observations are then removed from the dataframe 
algae.clean=algae[-(manyNAs(algae, 0.2)),]
```

Step 1.2: Imputation  
There are different type of imputation. The type I use here is called K Nearest Neighbor Imputation (KNN Imputation).   
KNN Imputation fills in the missing values by drawing on values of the K 'nearest neighbors' defined above. K is a value supplied by the user denoting the number of nearest neighbors to be used for missing value inference. For example, k=5, 10 etc.   
This process in R is accomplished by a function named "knnImputation". meth describes the kind of method used for inference. In this example, the method of choice is median and k=10.  
In other words, for every missing value, this method will choose the 10 nearest observations and the missing value would be estimated as their median values. The cleaned up data is stored in a new dataframe object named 'algae.clean.2' 
```{r step1.2, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
# Imputing to infer missing values
algae.clean.2 = knnImputation(algae.clean, k=10, meth="median") 
```

Now that missing value issue has been addressed using knnImputation, we can confirm it through using summary function if NA values have been replaced
```{r verification, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
# Checking for missing observations
summary(algae.clean.2) 
# yup, solved.
```

***************************************** End of Step 1. Clean-up dataset ***********************************************

## Step 2. Exploratory Data Analysis (EDA)
Since the data has been cleaned-up, we can now conduct some preliminary exploratory data analysis to understand the relationships between variables in the data set. 

The plot below shows that, of the 200 observations, slightly over 50% observations have target variable,a1 level, is between 0-10 while the remaining observations' a1 levels are spread between 10-100
```{r, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
# Histogram (a1 levels). 
library(ggplot2)
ggplot(algae.clean.2, aes(x=`a1`)) + geom_histogram(binwidth=10, fill="grey", color="red", boundary=0)+ 
scale_x_continuous(name="Algae Level (a1)", breaks=seq(0,100,10))+ scale_y_continuous(name="Count")+
theme(axis.text.x=element_text(size=14, face="bold"), axis.text.y=element_text(size=14, face="bold"),axis.title.x = element_text(size=14, face= "bold"), axis.title.y = element_text(size=12, face= "bold"))
```



It appears the variable "size" has a stronger influence on the al level than "season". 
```{r, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
par(mfrow=c(1,2)) # partition the plot area to accomodate two images

# Effect of season over a1 levels
ggplot(algae.clean.2, aes(x=`season`, y=`a1`)) + 
geom_boxplot()+
theme(axis.text.x=element_text(size=14, face="bold"), axis.text.y=element_text(size=14, face="bold"),axis.title.x = element_text(size=14, face= "bold"), axis.title.y = element_text(size=12, face= "bold"))

# Effect of size over a1 levels
ggplot(algae.clean.2, aes(x=`size`, y=`a1`)) + 
geom_boxplot()+
theme(axis.text.x=element_text(size=14, face="bold"), axis.text.y=element_text(size=14, face="bold"),axis.title.x = element_text(size=14, face= "bold"), axis.title.y = element_text(size=12, face= "bold"))
```



It also appears that the speed may not have strong influence on the a1 level
```{r, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
# Density plot
ggplot(algae.clean.2, aes(x=`a1`, fill=speed))+
geom_density(alpha=0.3)+scale_x_continuous(name="a1 level")+
theme(axis.title.x = element_text(size=14, face= "bold"), axis.title.y = element_text(size=12, face= "bold"))
```



Scatter plot is a good way to quickly scan variable relationship among numerical variables.  
It appears from the scatterplot below:  
1. mnO2 seems to have a positive relationship with a1.  
2. OPO4, PO4 and Chla: each have an inverse relationship with a1 
```{r, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
library(gcookbook)
pairs(algae.clean.2[,4:12])
```

**************************************** End of Step 2. EDA **********************************************************************8

## Step 3.1. Variable Selection Step

As described in the introduction, one of the key steps towards variable selection is to generate a linear regression model based on the variables.  

Notes about algae dataset:
1. The first 11 columns of the object represent the predictor/feature variables such as season, size, speed, mxPH, mnO2, Cl, NO3, NH4, OPO4, PO4 and chla
2. The 12th column represents the target variable, algae level a1. 
3. In other words, the goal is to predict algae level based on environmental factors and chemical concentrations.

```{r main.effect.model, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}

# lm is the linear model function
# formula is the formula of the linear model expressed in the following format: "target variable ~ function (predictor variable)
# In the case of formula = "target variable ~ . ", "." means all the available predictor variables.
# algae.lm1 represents the object that contains information about the linear model

algae.lm1 = lm(a1 ~ ., data=algae.clean.2[,1:12]) 
```

How to extract information about the linear model object, algae.lm1?  
1. Summary() below gives the coefficients of the various predictor variables in the linear model, their standard errors and significance.  
2. Significance indicates whether the relationship between a given predictor variable and the target variable is significantly correlated.  
3. The summary also provides the value of estimated adjusted regression coefficient which is useful to know how much of the data is explained by the model. 
4. Variable significance is indicated by * (p-value<0.05) or ** (p-value<0.01) or *** (p-value <0.001);   
5. p-value was arrived at based on t-value = Estimate/Std.Error) and we can see that a few variables such as size(small) and No3 significantly contribute to the mathematical model. This step of variable selection, therefore, helps to identify these significant variables that can then be used to construct the machine learning mathematical model in Step 3.2
```{r}
# Model information contained in the model object algae.lm1 can be extracted in the following way:
summary(algae.lm1)
```

Additional thoughts:
Another useful piece of information that can be analyzed from the model object is the residual data generated from the model. Residual data represents how much the predicted model output of target variable differs from the actual data. Using the residual information, one can infer information such as whether there is normality in the data as well as detecting outlier observations in the dataset.
```{r analyzing.residuals.plot, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
par(mfrow=c(2,2)) # this command creates a 2 by 2 spread for the four output images generated by the next plot command 
plot(algae.lm1)
```

Of these four, the 2 plots - **Normal Q-Q plot and  Residual v. Leverage plot**, are particularly important.  
1. If the points on the Q-Q normality plot deviates from the dotted line, then the inference is that the residuals do not form a Gaussian/normal distribution and therefore the model (Linear model in the present case) is not a good fit. In the current example, it seems to be non-gaussian for target values (a1 values) on the higher side.  
2. On the other hand, the leverage value of observations in the residual vs leverage plot shows how much leverage the observation has on affecting the model parameter values. In other words, a high leverage value for an observation beyond a certain threshold (called cooks's distance shown by red dotted lines in the graph) indicates an outlier. In the present case, observation # 153 qualifies as an outlier and hence will be removed from the dataset.

```{r echo=TRUE}
algae.clean.2.1 = algae.clean.2[-153,]
```

Now that outliers have been eliminated from data, one of the first things to do is to split data to test data and training data because:  
(i) Training data is used for variable selection and mathematical model development  
(ii) Test data should be purely used for final validation of the generated model to prevent learning bias creeping up inside the model

```{r train.and.test, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
# I split the data set into train and test data by a 2:1 ratio 
# Since sampling is random, set.seed() ensures the same sample is drawn each time
set.seed(0) 
train.rows=sample (1:nrow(algae.clean.2.1), size=round(2/3*nrow(algae.clean.2)), replace =F) # 2/3 sample is drawn from the data set to form training data

# training and test data sets are assigned
algae.clean.2.1.train= algae.clean.2.1[train.rows,] 
algae.clean.2.1.test= algae.clean.2.1[-train.rows,]
```

Now having removed the outlier, the linear model function (lm) was executed again and the following was the output
```{r variable.analysis, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
algae.lm2=lm(formula=a1~., data=algae.clean.2.1.train[,1:12])
summary(algae.lm2)
```

Since we now know only a few variables are useful to create the model, a good next step would be to create a <b>backward stepwise regression model </b>. In other words, this is a step-by-step regression model creation process that gradually eliminates weak variables from the variable set leaving behind only the most effective variables to generate a strong model. The below output shows the gradual winnowing out of the weak variables.
```{r backward.stepwise, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
algae.step=step(algae.lm2) # it takes as input a linear model object
```
More explanation on the output of **stepwise regression model**:  
1. As you can see this was a step by step approach. The process starts with all variables in the linear model, then each variable is left out while the other variables are retained in the model and this process is conducted for each variable and the resulting models are ordered based on their increasing AIC values.   
2. AIC values are based on the degrees of fit of the data achieved by each model. Among the various models, the model with the lowest AIC value is chosen for the next step as lower the AIC value, the better. 
3. The same process is conducted on the chosen model and this process is carried on till the best model to the next step is same as the starting model at that step. At that point, that last model becomes the optimal linear model and the variables forming that model are the best variables for the model.

```{r summary.backward.stepwise, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
# The summary of the final optimal linear model from the above backward stepwise regression process is extracted as shown below.
summary(algae.step)
```

Based on the summary of stepwise regression, the variables that are most useful/significant in a linear model to predict algae levels are:  
1. size (small)  
2. NO3  
3. PO4



### Subset Modeling
It is also a good idea to arrive at the optimal list of variables for generating a model through another approach called "subset modeling". Program code for subset modeling is included in "Leaps", an R code package. Different combinations/subsets of variables are tested by subset modeling approach and the models are arranged in the order of decreasing adjusted Regression Coefficient (R2) values. Once the models are sorted according to their R2 values, the results are shown visually with a clear indication of variables constituting each model. Based on the R2 values,one can narrow down the optimal subset of variables that can produce the best model.

```{r subset.modeling, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
library(leaps)

# nbest=10 means 10 best combinations of variables are selected for each model size (model size = # of variables). 
algae.sub=regsubsets(a1~., data=algae.clean.2.1.train[,1:12], nbest=10) 

# plotting the results of subset regression modeling
plot(algae.sub, scale="adjr2")
```

The result of subset modeling hint that the following variables are optimal for a linear model to predict algae levels:  
1. size (small)  
2. mxPH  
3. mnO2  
4. NO3  
5. PO4  
6. chla


### Cross-Validation 
If you want to avoid over-fitting the model during the training phase, there is a neat approach called "Cross-Validation (CV)". The principle of cross-validation is to randomly divide the data in to "m" more or less equally sized data fragments (The value of m" is chosen by the user. The name of the cross-validation approach is then called M-fold CV). The process uses use m-1 data fragments to train the model and then the trained model is tested on the one remaining data fragment to measure the degree of fit for the model. This process is then repeated by cycling through every data fragment and averaging the degrees of fits of the model across all those fragments. In this way, since the model is subjected to rigorous training, we would avoid over-fitting the model and the true accuracy of the model is measured by averaging the model's performance on the various test data fragments.

Additional programming notes:       
1. CVlm() is the function to carry out linear model cross validation present in the R code package, DAAG  
2. The "form.lm" argument in CVlm() stands for linear model formula  
3. "m" stands for the number of folds of cross validation the user desires.  
4. The output shows the overall ANOVA table displaying significance of the different variables, the observations used in the different test data fragments (also called "folds"), mean sum of square of errors/residuals for each test data fragment and overall performance, that is, average mean sum of square of errors across all the folds.  
5. The CV plot shows how well the linear model predicted a1 values matched with the actual a1 values. The closer the points are to the 45 degree line, closer are the predictions to the actual values. Based on the plot generated, there is a lot of disparity between predictions and actual values. 

```{r cross-validation, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
library(DAAG) 
CVlm(data=algae.clean.2.1.train[,1:12], form.lm=formula(algae.lm2), m=10)
```

Based on the ANOVA table above, many variables have been selected as significant by the CV approach.


### Bootstrapping
Another way to conduct variable selection is to assess the relative importance of the different variables of the model and estimate how much does a particular variable contribute to the model's overall accuracy/regression coefficient. This is accomplished by a process called "Bootstrapping". The individual contributions of variables to the overall response variance of the model is determined by adding one variable at a time to the model and observing how much improvement it adds to the model's accuracy. This is done by repeating the model construction process many times for each variable ("b" = input from the user denoting how many times to re-sample/bootstrap for each variable) by choosing different orders for the variable in question because it matters when the variable gets added to the model - first, middle (lmg) or last. Therefore different variables are added in different orders to the model with respect to the variable of interest during the "b" bootstraps and the average improvement to a model's accuracy generated by the variable of interest based on when it was introduced to the model (first, lmg or last) is estimated along with a confidence interval and these results for each variable is plotted along with the information of when the variable was inserted into the model. Below code fragment shows how bootstrapping is conducted in R for our current linear model using the algae dataset. 

```{r bootstrapping, echo=TRUE, warning=FALSE, message=FALSE, tidy=TRUE, cache=TRUE, comment=""}
library(relaimpo)  # the code package that has the instructions to conduct bootstrapping process.

# The function boot.relimp in "relaimpo" package takes as input the linear model that has already been constructed to conduct bootstrap on. # The function also takes as input the different times/types to introduce the variable to the model (first, middle or last)

algae.boot=boot.relimp(b=100, algae.lm2, type=c("lmg", "first", "last") )

plot(booteval.relimp(algae.boot, sort=TRUE))

```
The plot above shows the relative contribution of each variable to the overall linear model performance (R2 ~ 37%). For example, the variable, PO4, contributes nearly R2 = 25% when added as the first variable to the linear model. On the other hand, the same variable when added last to the model contributes only R2=2% to the model. Based on the above plot, I would choose the following variables to generate a strong model: PO4, oPO4, Cl, NO3, size

Having understood the data well due to preliminary analysis and a few variable selection techniques, I select the following list of  variables to be used for developing a model in Step 3.2:  
<b>
1.size (small)  
2.mxPH  
3. mnO2  
4. NO3  
5. PO4  
6. oPO4  
7. Cl
</b>
 
